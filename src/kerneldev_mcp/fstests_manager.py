"""
Core fstests management - installation, configuration, execution, and result parsing.
"""
import json
import logging
import os
import re
import subprocess
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime

logger = logging.getLogger(__name__)


@dataclass
class FstestsConfig:
    """Configuration for fstests."""

    fstests_path: Path
    test_dev: str
    test_dir: Path
    scratch_dev: str
    scratch_dir: Path
    fstype: str
    mount_options: Optional[str] = None
    mkfs_options: Optional[str] = None
    additional_vars: Dict[str, str] = field(default_factory=dict)

    def to_config_text(self) -> str:
        """Generate local.config file content.

        Returns:
            Configuration file content
        """
        lines = [
            "# fstests local.config",
            f"# Generated by kerneldev-mcp on {datetime.now().isoformat()}",
            "",
            "# Required: Test device and mount point",
            f"export TEST_DEV={self.test_dev}",
            f"export TEST_DIR={self.test_dir}",
            "",
            "# Required: Scratch device for destructive tests",
            f"export SCRATCH_DEV={self.scratch_dev}",
            f"export SCRATCH_MNT={self.scratch_dir}",
            "",
            "# Filesystem type",
            f"export FSTYP={self.fstype}",
            ""
        ]

        # Add optional mount options
        if self.mount_options:
            lines.append("# Mount options")
            lines.append(f'export MOUNT_OPTIONS="{self.mount_options}"')
            lines.append("")

        # Add optional mkfs options
        if self.mkfs_options:
            lines.append("# mkfs options")
            lines.append(f'export MKFS_OPTIONS="{self.mkfs_options}"')
            lines.append("")

        # Add additional variables
        if self.additional_vars:
            lines.append("# Additional configuration")
            for key, value in self.additional_vars.items():
                lines.append(f'export {key}="{value}"')
            lines.append("")

        return "\n".join(lines)


@dataclass
class TestResult:
    """Result of a single test."""

    test_name: str
    status: str  # "passed", "failed", "notrun"
    duration: float  # seconds
    failure_reason: Optional[str] = None
    output_file: Optional[Path] = None
    full_log: Optional[Path] = None


@dataclass
class FstestsRunResult:
    """Result of an fstests run."""

    success: bool
    total_tests: int
    passed: int
    failed: int
    notrun: int
    test_results: List[TestResult] = field(default_factory=list)
    duration: float = 0.0
    log_file: Optional[Path] = None
    check_log: Optional[Path] = None

    @property
    def pass_rate(self) -> float:
        """Calculate pass rate."""
        if self.total_tests == 0:
            return 0.0
        return (self.passed / self.total_tests) * 100

    def summary(self) -> str:
        """Get human-readable summary.

        Returns:
            Summary string
        """
        if self.total_tests == 0:
            return "No tests run"

        status_icon = "✓" if self.failed == 0 else "✗"
        summary = f"{status_icon} {self.passed}/{self.total_tests} passed"

        if self.failed > 0:
            summary += f", {self.failed} failed"
        if self.notrun > 0:
            summary += f", {self.notrun} not run"

        summary += f" ({self.pass_rate:.1f}% pass rate, {self.duration:.1f}s)"

        return summary


class FstestsManager:
    """Manages fstests installation, configuration, and execution."""

    # Default git repository
    DEFAULT_GIT_URL = "git://git.kernel.org/pub/scm/fs/xfs/xfstests-dev.git"

    # Test group descriptions
    TEST_GROUPS = {
        "auto": "Tests suitable for automatic testing (excludes dangerous tests)",
        "quick": "Fast tests for smoke testing (~5-10 minutes)",
        "all": "All tests including dangerous ones",
        "dangerous": "Tests that may crash/hang the kernel",
        "stress": "Stress tests (long running)",
        "aio": "Async I/O tests",
        "attr": "Extended attributes",
        "acl": "Access control lists",
        "quota": "Quota tests",
        "encrypt": "Encryption tests",
        "compress": "Compression tests",
    }

    @staticmethod
    def validate_test_args(tests: List[str]) -> Tuple[bool, Optional[str]]:
        """Validate test arguments for correct usage.

        Checks that:
        - "-g" flag is only used with group names, not individual test names
        - Individual test names (category/number format) are not used with "-g"

        Args:
            tests: List of test arguments (e.g., ["-g", "quick"] or ["btrfs/010"])

        Returns:
            Tuple of (is_valid, error_message)
        """
        if not tests:
            return True, None

        # Pattern for individual test names: category/number (e.g., btrfs/010, generic/001)
        test_name_pattern = re.compile(r'^[a-z0-9_]+/\d+$')

        i = 0
        while i < len(tests):
            arg = tests[i]

            # Check if this is a "-g" flag
            if arg == "-g":
                # Make sure there's a next argument
                if i + 1 >= len(tests):
                    return False, "'-g' flag requires a group name argument"

                next_arg = tests[i + 1]

                # Check if the next argument looks like an individual test name
                if test_name_pattern.match(next_arg):
                    return False, (
                        f"Invalid test argument: '-g {next_arg}'. "
                        f"The '-g' flag is for groups (like 'quick', 'auto'), not individual tests. "
                        f"To run individual test '{next_arg}', use it without '-g': ['{next_arg}']"
                    )

                i += 2  # Skip both -g and its argument
            else:
                i += 1

        return True, None

    def __init__(self, fstests_path: Optional[Path] = None):
        """Initialize fstests manager.

        Args:
            fstests_path: Path to fstests installation (default: ~/.kerneldev-mcp/fstests)
        """
        if fstests_path is None:
            default_path = Path.home() / ".kerneldev-mcp" / "fstests"
            self.fstests_path = default_path
        else:
            self.fstests_path = Path(fstests_path)

    def check_installed(self) -> bool:
        """Check if fstests is installed.

        Returns:
            True if fstests is installed and built
        """
        # Check if directory exists
        if not self.fstests_path.exists():
            return False

        # Check for check script
        check_script = self.fstests_path / "check"
        if not check_script.exists():
            return False

        # Check if built (common binary exists)
        src_dir = self.fstests_path / "src"
        if not src_dir.exists():
            return False

        # Check for at least one compiled binary
        for binary in ["fsstress", "aio-dio-regress", "t_mtab"]:
            if (src_dir / binary).exists():
                return True

        return False

    def get_version(self) -> Optional[str]:
        """Get fstests version.

        Returns:
            Version string or None if not available
        """
        if not self.check_installed():
            return None

        try:
            # Get git describe version
            result = subprocess.run(
                ["git", "describe", "--always"],
                cwd=self.fstests_path,
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.returncode == 0:
                return result.stdout.strip()

            # Fallback: get commit hash
            result = subprocess.run(
                ["git", "rev-parse", "--short", "HEAD"],
                cwd=self.fstests_path,
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.returncode == 0:
                return result.stdout.strip()

        except (subprocess.TimeoutExpired, subprocess.CalledProcessError):
            pass

        return None

    def check_build_dependencies(self) -> Tuple[bool, List[str]]:
        """Check for common build dependencies.

        Returns:
            Tuple of (all_found, missing_tools_and_packages)
        """
        required_tools = ["make", "gcc", "git"]
        missing = []

        # Check for command-line tools
        for tool in required_tools:
            try:
                subprocess.run(
                    [tool, "--version"],
                    capture_output=True,
                    timeout=5
                )
            except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
                missing.append(tool)

        # Check for development packages by looking for required header files
        # These are needed for fstests to build successfully
        required_headers = {
            "xfs/xfs.h": "xfsprogs-devel (Fedora/RHEL) or xfslibs-dev (Debian/Ubuntu)",
            "sys/acl.h": "libacl-devel (Fedora/RHEL) or libacl1-dev (Debian/Ubuntu)",
            "attr/xattr.h": "libattr-devel (Fedora/RHEL) or libattr1-dev (Debian/Ubuntu)",
        }

        for header, package_info in required_headers.items():
            # Try to compile a simple test program that includes this header
            test_program = f'#include <{header}>\nint main() {{ return 0; }}'
            try:
                result = subprocess.run(
                    ["gcc", "-x", "c", "-c", "-o", "/dev/null", "-"],
                    input=test_program,
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                if result.returncode != 0:
                    missing.append(f"{header} ({package_info})")
            except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
                missing.append(f"{header} ({package_info})")

        return (len(missing) == 0), missing

    def install(self, git_url: Optional[str] = None, check_dependencies: bool = True) -> Tuple[bool, str]:
        """Install fstests from git.

        Args:
            git_url: Git repository URL (default: kernel.org)
            check_dependencies: Whether to check for required dependencies

        Returns:
            Tuple of (success, message)
        """
        git_url = git_url or self.DEFAULT_GIT_URL

        # Check dependencies if requested
        if check_dependencies:
            deps_ok, missing = self.check_build_dependencies()
            if not deps_ok:
                return False, (
                    f"Missing required build tools: {', '.join(missing)}\n\n"
                    "Install with:\n"
                    "  Ubuntu/Debian: sudo apt-get install build-essential git\n"
                    "  Fedora/RHEL: sudo dnf install gcc make git"
                )

        # Create parent directory
        self.fstests_path.parent.mkdir(parents=True, exist_ok=True)

        # Clone repository
        try:
            result = subprocess.run(
                ["git", "clone", git_url, str(self.fstests_path)],
                capture_output=True,
                text=True,
                timeout=300
            )
            if result.returncode != 0:
                return False, f"Git clone failed: {result.stderr}"
        except subprocess.TimeoutExpired:
            return False, "Git clone timed out after 5 minutes"
        except Exception as e:
            return False, f"Git clone error: {str(e)}"

        # Build fstests
        success, message = self.build()
        if not success:
            # Add dependency hint to build failures
            hint = (
                "\n\nIf build fails due to missing libraries, install fstests dependencies:\n"
                "  Ubuntu/Debian: sudo apt-get install -y xfslibs-dev uuid-dev libtool-bin \\\n"
                "    e2fsprogs automake gcc libuuid1 quota attr libattr1-dev make \\\n"
                "    libacl1-dev libaio-dev xfsprogs libgdbm-dev gawk fio dbench \\\n"
                "    uuid-runtime liburing-dev libcap-dev\n"
                "  Fedora/RHEL: sudo dnf install -y acl attr automake bc dbench dump \\\n"
                "    e2fsprogs fio gawk gcc indent libtool lvm2 make psmisc quota sed \\\n"
                "    xfsdump xfsprogs libacl-devel libattr-devel libaio-devel \\\n"
                "    libuuid-devel xfsprogs-devel libcap-devel liburing-devel"
            )
            return False, f"{message}{hint}"

        return True, f"Successfully installed fstests to {self.fstests_path}"

    def build(self) -> Tuple[bool, str]:
        """Build fstests.

        Returns:
            Tuple of (success, message)
        """
        if not self.fstests_path.exists():
            return False, f"fstests directory does not exist: {self.fstests_path}"

        # Check if configure script exists and run it
        configure_script = self.fstests_path / "configure"
        builddefs_file = self.fstests_path / "include" / "builddefs"

        if configure_script.exists():
            try:
                result = subprocess.run(
                    ["./configure"],
                    cwd=self.fstests_path,
                    capture_output=True,
                    text=True,
                    timeout=120
                )

                # Check if configure actually succeeded
                if result.returncode != 0:
                    error_msg = "Configure failed.\n\n"

                    # Check for common errors in configure output
                    output = result.stdout + result.stderr
                    if "xfs/xfs.h" in output or "FATAL ERROR" in output:
                        error_msg += "Missing required development packages.\n\n"
                        error_msg += "Install dependencies:\n"
                        error_msg += "  Fedora/RHEL: sudo dnf install -y xfsprogs-devel libacl-devel libattr-devel \\\n"
                        error_msg += "    libaio-devel liburing-devel libcap-devel\n"
                        error_msg += "  Ubuntu/Debian: sudo apt-get install -y xfslibs-dev libacl1-dev libattr1-dev \\\n"
                        error_msg += "    libaio-dev liburing-dev libcap-dev\n\n"

                    # Include last part of configure output
                    if result.stderr:
                        error_msg += "Configure error output:\n" + result.stderr[-1000:]
                    elif result.stdout:
                        error_msg += "Configure output:\n" + result.stdout[-1000:]

                    return False, error_msg

                # Verify that configure created the required builddefs file
                if not builddefs_file.exists():
                    return False, (
                        "Configure completed but failed to create include/builddefs.\n"
                        "This usually means configure was interrupted or failed silently.\n"
                        "Check that all development packages are installed."
                    )

            except subprocess.TimeoutExpired:
                return False, "Configure timed out after 2 minutes"
            except Exception as e:
                return False, f"Configure error: {str(e)}"

        # Run make
        try:
            result = subprocess.run(
                ["make", f"-j{os.cpu_count() or 1}"],
                cwd=self.fstests_path,
                capture_output=True,
                text=True,
                timeout=300
            )

            if result.returncode != 0:
                # Build failed, provide detailed error
                error_msg = "Build failed.\n\n"
                if result.stderr:
                    error_msg += "Error output:\n" + result.stderr[:1000]
                if result.stdout:
                    error_msg += "\n\nBuild output:\n" + result.stdout[:1000]
                return False, error_msg

        except subprocess.TimeoutExpired:
            return False, "Build timed out after 5 minutes"
        except Exception as e:
            return False, f"Build error: {str(e)}"

        # Verify that critical binaries were built
        critical_binaries = ["ltp/fsstress", "src/aio-dio-regress"]
        missing_binaries = []
        for binary in critical_binaries:
            binary_path = self.fstests_path / binary
            if not binary_path.exists() or not os.access(binary_path, os.X_OK):
                missing_binaries.append(binary)

        if missing_binaries:
            return False, (
                f"Build completed but critical binaries were not created: {', '.join(missing_binaries)}\n"
                "This usually means the build failed silently. Check that all dependencies are installed."
            )

        return True, "Build successful"

    def write_config(self, config: FstestsConfig) -> bool:
        """Write local.config file.

        Args:
            config: FstestsConfig to write

        Returns:
            True if successful
        """
        if not self.fstests_path.exists():
            return False

        config_file = self.fstests_path / "local.config"

        try:
            config_file.write_text(config.to_config_text())
            return True
        except OSError:
            return False

    def parse_check_output(self, output: str, check_log: Optional[Path] = None) -> FstestsRunResult:
        """Parse output from ./check command.

        Args:
            output: Output from check command (used as fallback)
            check_log: Path to check.log file (preferred source)

        Returns:
            FstestsRunResult with parsed results
        """
        test_results = []
        passed = 0
        failed = 0
        notrun = 0

        # Prefer reading from check.log file if available (cleaner output)
        if check_log and check_log.exists():
            try:
                with open(check_log, 'r') as f:
                    full_log = f.read()

                # check.log contains multiple test runs (append-only)
                # Split by empty lines and take the last entry
                entries = full_log.strip().split('\n\n')
                if entries:
                    output = entries[-1]  # Use only the most recent run
            except Exception:
                # Fall back to parsing console output if file read fails
                pass

        # Clean output by removing kernel log timestamps that might be interleaved
        # Pattern: [    1.234567] at start of line or embedded in line
        cleaned_lines = []
        for line in output.splitlines():
            # Remove kernel timestamps from the line
            cleaned = re.sub(r'\[\s*\d+\.\d+\].*?(?=\S|$)', '', line)
            cleaned_lines.append(cleaned.strip())

        # Join consecutive lines that might have been split by kernel messages
        # Look for test name followed by duration on next line
        merged_lines = []
        i = 0
        while i < len(cleaned_lines):
            line = cleaned_lines[i]
            # Check if this line is a test name and next line is just a duration
            if i + 1 < len(cleaned_lines):
                next_line = cleaned_lines[i + 1]
                # If current line looks like a test name and next line is just "Xs"
                if re.match(r'^\S+/\d+\s*$', line) and re.match(r'^\s*\d+s\s*$', next_line):
                    merged_lines.append(f"{line} {next_line.strip()}")
                    i += 2
                    continue
            merged_lines.append(line)
            i += 1

        # Parse each line
        for line in merged_lines:
            if not line:
                continue

            # Match test results
            # Examples:
            # generic/001 5s
            # generic/002  [not run] requires feature
            # generic/003 - output mismatch (see generic/003.out.bad)

            # Passed test (allow for extra whitespace)
            match = re.match(r'^(\S+)\s+(\d+)s$', line.strip())
            if match:
                test_name = match.group(1)
                duration = float(match.group(2))
                test_results.append(TestResult(
                    test_name=test_name,
                    status="passed",
                    duration=duration
                ))
                passed += 1
                continue

            # Not run test
            match = re.match(r'^(\S+)\s+\[not run\]\s*(.*)$', line.strip())
            if match:
                test_name = match.group(1)
                reason = match.group(2).strip()
                test_results.append(TestResult(
                    test_name=test_name,
                    status="notrun",
                    duration=0.0,
                    failure_reason=reason if reason else None
                ))
                notrun += 1
                continue

            # Failed test
            match = re.match(r'^(\S+)\s+-\s+(.*)$', line.strip())
            if match:
                test_name = match.group(1)
                reason = match.group(2).strip()

                # Try to find .out.bad file
                output_file = None
                if "see" in reason:
                    # Extract filename from "see XXX.out.bad"
                    file_match = re.search(r'see (\S+\.out\.bad)', reason)
                    if file_match:
                        output_file = self.fstests_path / "results" / file_match.group(1)

                test_results.append(TestResult(
                    test_name=test_name,
                    status="failed",
                    duration=0.0,
                    failure_reason=reason,
                    output_file=output_file if output_file and output_file.exists() else None
                ))
                failed += 1
                continue

        # If we didn't find any test results via line-by-line parsing, try to parse summary lines
        # This handles cases where kernel messages completely obscured the detailed results
        if len(test_results) == 0:
            # Look for "Ran: test/name test/name2 ..." or "Ran: test/name"
            ran_match = re.search(r'Ran:\s+(.+)', output)
            if ran_match:
                test_names_line = ran_match.group(1).strip()
                # Remove "tests in Xs" suffix if present
                test_names_line = re.sub(r'\s+tests?\s+in\s+\d+s.*$', '', test_names_line)
                # Split by whitespace to get individual test names
                test_names = [name for name in test_names_line.split() if '/' in name]

                # Look for "Not run: test/name" lines first
                notrun_tests = set()
                for line in output.splitlines():
                    notrun_match = re.match(r'Not run:\s+(.+)', line.strip())
                    if notrun_match:
                        notrun_names = [name for name in notrun_match.group(1).split() if '/' in name]
                        notrun_tests.update(notrun_names)

                # Look for "Failures: test/name" line
                failed_tests = set()
                for line in output.splitlines():
                    failed_match = re.match(r'Failures:\s+(.+)', line.strip())
                    if failed_match:
                        failed_names = [name for name in failed_match.group(1).split() if '/' in name]
                        failed_tests.update(failed_names)

                # Process each test based on its status
                for test_name in test_names:
                    if test_name in notrun_tests:
                        test_results.append(TestResult(
                            test_name=test_name,
                            status="notrun",
                            duration=0.0,
                            failure_reason="Test not run (check requirements)"
                        ))
                        notrun += 1
                    elif test_name in failed_tests:
                        test_results.append(TestResult(
                            test_name=test_name,
                            status="failed",
                            duration=0.0,
                            failure_reason="Test failed (see logs for details)"
                        ))
                        failed += 1
                    else:
                        # Assume passed if not in notrun or failed lists
                        test_results.append(TestResult(
                            test_name=test_name,
                            status="passed",
                            duration=0.0
                        ))
                        passed += 1

        total_tests = len(test_results)

        # Extract total duration if available
        # Matches "Ran: 4 tests in 15s" or "Ran: 4 in 15s"
        duration_match = re.search(r'Ran:\s+.*?\s+in\s+(\d+)s', output)
        total_duration = float(duration_match.group(1)) if duration_match else 0.0

        # Check for error messages that indicate command failure
        # These should mark the run as failed even if no individual test failures were detected
        error_patterns = [
            r'Group\s+"[^"]+"\s+is empty or not defined',  # Invalid group name
            r'Usage:.*check',  # Usage message (invalid arguments)
            r'check:\s+invalid option',  # Invalid option
            r'ERROR:',  # Generic error
        ]

        has_error_message = any(re.search(pattern, output, re.IGNORECASE) for pattern in error_patterns)

        # If we found error messages and no tests ran, this is a command failure
        if has_error_message and total_tests == 0:
            success = False
        else:
            success = (failed == 0)

        return FstestsRunResult(
            success=success,
            total_tests=total_tests,
            passed=passed,
            failed=failed,
            notrun=notrun,
            test_results=test_results,
            duration=total_duration,
            check_log=check_log
        )

    def run_tests(
        self,
        tests: Optional[List[str]] = None,
        exclude_file: Optional[Path] = None,
        randomize: bool = False,
        iterations: int = 1,
        timeout: Optional[int] = None
    ) -> FstestsRunResult:
        """Run fstests.

        Args:
            tests: List of tests to run (e.g., ["generic/001"] or ["-g", "quick"])
            exclude_file: Path to exclude file
            randomize: Randomize test order
            iterations: Number of times to run tests
            timeout: Timeout in seconds

        Returns:
            FstestsRunResult with test results
        """
        logger.info("=" * 60)
        logger.info(f"Starting fstests: {self.fstests_path}")

        # Validate test arguments
        if tests:
            is_valid, error_msg = self.validate_test_args(tests)
            if not is_valid:
                logger.error(f"✗ Invalid test arguments: {error_msg}")
                logger.info("=" * 60)
                return FstestsRunResult(
                    success=False,
                    total_tests=0,
                    passed=0,
                    failed=0,
                    notrun=0,
                    test_results=[],
                    duration=0.0
                )

        if not self.check_installed():
            logger.error("✗ fstests not installed")
            logger.info("=" * 60)
            return FstestsRunResult(
                success=False,
                total_tests=0,
                passed=0,
                failed=0,
                notrun=0,
                test_results=[],
                duration=0.0
            )

        # Build check command
        cmd = ["sudo", "./check"]

        # Add test selection
        if tests:
            cmd.extend(tests)
            logger.info(f"Test selection: {' '.join(tests)}")
        else:
            # Default to quick group
            cmd.extend(["-g", "quick"])
            logger.info("Test selection: -g quick (default)")

        # Add exclude file
        if exclude_file and exclude_file.exists():
            cmd.extend(["-E", str(exclude_file)])
            logger.info(f"Exclude file: {exclude_file}")

        # Randomize order
        if randomize:
            cmd.append("-r")
            logger.info("Randomized test order enabled")

        # Iterations
        if iterations > 1:
            cmd.extend(["-i", str(iterations)])
            logger.info(f"Running {iterations} iterations")

        # Run tests
        logger.info("Running fstests... (this may take several minutes)")
        start_time = time.time()

        try:
            result = subprocess.run(
                cmd,
                cwd=self.fstests_path,
                capture_output=True,
                text=True,
                timeout=timeout
            )

            duration = time.time() - start_time

            # Parse output
            check_log = self.fstests_path / "results" / "check.log"
            fstests_result = self.parse_check_output(result.stdout, check_log if check_log.exists() else None)
            fstests_result.duration = duration

            # Log results
            if fstests_result.success:
                logger.info(f"✓ fstests completed in {duration:.1f}s")
            else:
                logger.error(f"✗ fstests completed with failures in {duration:.1f}s")
            logger.info(f"  Total: {fstests_result.total_tests}, Passed: {fstests_result.passed}, "
                       f"Failed: {fstests_result.failed}, Not run: {fstests_result.notrun}")
            # Log first few failures
            failed_tests = [t for t in fstests_result.test_results if t.status == "failed"]
            for i, test in enumerate(failed_tests[:3]):
                logger.error(f"  Failed: {test.test_name}")
            if len(failed_tests) > 3:
                logger.error(f"  ... and {len(failed_tests) - 3} more failures")
            logger.info("=" * 60)

            return fstests_result

        except subprocess.TimeoutExpired as e:
            duration = time.time() - start_time
            logger.error(f"✗ fstests timeout after {timeout}s (ran for {duration:.1f}s)")
            logger.info("=" * 60)

            # Try to parse partial output
            output = e.stdout.decode() if isinstance(e.stdout, bytes) else (e.stdout or "")
            fstests_result = self.parse_check_output(output)
            fstests_result.duration = duration
            fstests_result.success = False

            return fstests_result

        except Exception as e:
            logger.error(f"✗ fstests failed with exception: {e}")
            logger.info("=" * 60)
            return FstestsRunResult(
                success=False,
                total_tests=0,
                passed=0,
                failed=0,
                notrun=0,
                test_results=[],
                duration=time.time() - start_time
            )

    def get_test_failure_details(self, test_name: str) -> Optional[str]:
        """Get detailed failure information for a test.

        Args:
            test_name: Name of test (e.g., "generic/001")

        Returns:
            Failure details or None if not available
        """
        # Look for .out.bad file
        out_bad = self.fstests_path / "results" / f"{test_name}.out.bad"
        if not out_bad.exists():
            # Try with filesystem type prefix
            for fstype in ["btrfs", "ext4", "xfs", "generic"]:
                out_bad = self.fstests_path / "results" / fstype / f"{test_name.split('/')[-1]}.out.bad"
                if out_bad.exists():
                    break
            else:
                return None

        # Read .out.bad
        try:
            bad_output = out_bad.read_text()
        except OSError:
            return None

        # Try to get expected output
        test_file = self.fstests_path / "tests" / f"{test_name}.out"
        expected_output = ""
        if test_file.exists():
            try:
                expected_output = test_file.read_text()
            except OSError:
                pass

        # Generate diff
        if expected_output:
            details = f"Expected output:\n{expected_output}\n\n"
            details += f"Actual output:\n{bad_output}\n"
        else:
            details = f"Actual output:\n{bad_output}\n"

        return details

    def list_groups(self) -> Dict[str, str]:
        """List available test groups.

        Returns:
            Dictionary of group names to descriptions
        """
        return self.TEST_GROUPS.copy()


def format_fstests_result(result: FstestsRunResult, max_failures: int = 10) -> str:
    """Format fstests result for display.

    Args:
        result: FstestsRunResult to format
        max_failures: Maximum number of failures to show

    Returns:
        Formatted string
    """
    lines = []
    lines.append(result.summary())
    lines.append("")

    # Show failed tests
    failed_tests = [t for t in result.test_results if t.status == "failed"]
    if failed_tests:
        lines.append(f"Failed Tests ({len(failed_tests)}):")
        for i, test in enumerate(failed_tests[:max_failures], 1):
            lines.append(f"  {i}. {test.test_name}")
            if test.failure_reason:
                lines.append(f"     {test.failure_reason}")
        if len(failed_tests) > max_failures:
            lines.append(f"  ... and {len(failed_tests) - max_failures} more failures")
        lines.append("")

    # Show not run tests (only first few)
    notrun_tests = [t for t in result.test_results if t.status == "notrun"]
    if notrun_tests:
        lines.append(f"Not Run ({len(notrun_tests)}):")
        for i, test in enumerate(notrun_tests[:5], 1):
            lines.append(f"  {i}. {test.test_name}")
            if test.failure_reason:
                lines.append(f"     {test.failure_reason}")
        if len(notrun_tests) > 5:
            lines.append(f"  ... and {len(notrun_tests) - 5} more")
        lines.append("")

    # Show log file location
    if result.check_log:
        lines.append(f"Full results: {result.check_log}")

    return "\n".join(lines)
